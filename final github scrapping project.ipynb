{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the top repositories for topics of github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO (Intro):\n",
    "\n",
    "Introduction about web scraping\n",
    "Introduction about GitHub and the problem statement\n",
    "Use of Python, requests, Beautiful Soup, Pandas\n",
    "\n",
    "Here are the steps we'll follow:\n",
    "\n",
    "We're going to scrape https://github.com/topics\n",
    "We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description\n",
    "For each topic, we'll get the top 25 repositories in the topic from the topic page\n",
    "For each repository, we'll grab the repo name, username, stars and repo URL\n",
    "For each topic we'll create a CSV file in the following format:\n",
    "Repo Name,Username,Stars,Repo URL\n",
    "three.js,mrdoob,69700,https://github.com/mrdoob/three.js\n",
    "libgdx,libgdx,18300,https://github.com/libgdx/libgdx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the list of topics from Github\n",
    "Explain how you'll do it.\n",
    "\n",
    "use requests to downlaod the page\n",
    "user BS4 to parse and extract information\n",
    "convert to a Pandas dataframe\n",
    "Let's write a function to download the page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "def get_topic_page():\n",
    "    #download the page\n",
    "    topic_url='https://github.com/topics'\n",
    "    response=requests.get(topic_url)\n",
    "    #check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page{}'.format(topic_url))\n",
    "    # parse using BeautifulSoup\n",
    "    topic_doc=BeautifulSoup(response.text,\"html.parser\")\n",
    "    return topic_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = get_topic_page()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some helper functions to parse information from the page.\n",
    "\n",
    "To get topic titles, we can pick p tags:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    topic_title_tags = doc.find_all('p', {'class': selection_class})\n",
    "    topic_titles = []\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3D', 'Ajax', 'Algorithm', 'Amp', 'Android']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_topic_titles can be used to get the list of titles\n",
    "titles = get_topic_titles(doc)\n",
    "len(titles)\n",
    "titles[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarly we have defined functions for descriptions and URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    discription_class={\"class\":\"f5 color-fg-muted mb-0 mt-1\"}\n",
    "    topic_discription_class=soup.find_all('p',discription_class)\n",
    "    topic_description=[]\n",
    "    for tag in topic_discription_class:\n",
    "        topic_description.append(tag.text.strip())\n",
    "    return topic_description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    topic_link_tags = soup.find_all('a',{'class': \"no-underline flex-grow-0\"})\n",
    "    topic_urls=[]\n",
    "    base_url=\"https://github.com\"\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag[\"href\"])\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A single function for all this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics():\n",
    "    topic_url='https://github.com/topics'\n",
    "    response=requests.get(topic_url)\n",
    "    #check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page{}'.format(topic_url))\n",
    "    # parse using BeautifulSoup\n",
    "    topic_doc=BeautifulSoup(response.text,\"html.parser\")\n",
    "    topic_dict={\"title\":get_topic_titles(topic_doc),\"description\":get_topic_descs(topic_doc),\"url\":get_topic_urls(topic_doc)}\n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the top 25 Repositories from a topic page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_topic_page(topic_url):\n",
    "    # Download the page\n",
    "    response = requests.get(topic_url)\n",
    "    # Check successful response\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    # Parse using Beautiful soup\n",
    "    topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_strip(star_str):\n",
    "    star_str=star_str.strip()\n",
    "    if star_str[-1]=='k':\n",
    "       return int(float(star_str[:-1])*1000)\n",
    "    return int(star_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repo_info(h3_tag,star_tag):  # gives all information about repository\n",
    "    base_url=\"https://github.com\"\n",
    "    a_tag=h3_tag.find_all('a')\n",
    "    user_name=a_tag[0].text.strip()\n",
    "    repo_name=a_tag[1].text.strip()\n",
    "    repo_url=base_url+a_tag[1]['href']\n",
    "    stars=parse_star_strip(star_tag.text.strip())\n",
    "    \n",
    "    return user_name,repo_name,repo_url,stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repos(topic_doc):\n",
    "    #get the h3 tag containing repo title,repo url,username\n",
    "    h3_selection_class=\"f3 color-fg-muted text-normal lh-condensed\"\n",
    "    repo_tag=topic_doc.find_all('h3',{'class':h3_selection_class})\n",
    "    # get star tags\n",
    "    star_tags=topic_doc.find_all('span',{'class':\"Counter js-social-count\"})\n",
    "    topic_repo_dict={\n",
    "    \"user_name\":[],\n",
    "    \"repo_name\":[],\n",
    "    \"repo_url\":[],\n",
    "    \"stars\":[]\n",
    "    }\n",
    "    # get repo info\n",
    "    for i in range(len(repo_tag)):\n",
    "      repo_info = get_repo_info(repo_tag[i],star_tags[i])\n",
    "      topic_repo_dict[\"user_name\"].append(repo_info[0])\n",
    "      topic_repo_dict[\"repo_name\"].append(repo_info[1])\n",
    "      topic_repo_dict[\"repo_url\"].append(repo_info[2])\n",
    "      topic_repo_dict[\"stars\"].append(repo_info[3])\n",
    "    return pd.DataFrame(topic_repo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(topic_url,topic_name):\n",
    "    fname=topic_name+'.csv'\n",
    "    if os.path.exists(fname):\n",
    "        print(\"The file {} already exists.Skipping...\".format(fname))\n",
    "        return\n",
    "    topic_df=get_topic_repos(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(fname,index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(soup):\n",
    "    topic_title_tags=soup.find_all('p',{\"class\":\"f3 lh-condensed mb-0 mt-1 Link--primary\"})\n",
    "    topic_titles=[]\n",
    "    \n",
    "    for tags in topic_title_tags:\n",
    "        topic_titles.append(tags.text)\n",
    "    return topic_titles\n",
    "\n",
    "def get_topic_descs(soup):\n",
    "     discription_class={\"class\":\"f5 color-fg-muted mb-0 mt-1\"}\n",
    "     topic_discription_class=soup.find_all('p',discription_class)\n",
    "     topic_description=[]\n",
    "     for tag in topic_discription_class:\n",
    "         topic_description.append(tag.text.strip())\n",
    "     return topic_description\n",
    "\n",
    "def get_topic_urls(soup):\n",
    "    topic_link_tags = soup.find_all('a',{'class': \"no-underline flex-grow-0\"})\n",
    "    topic_urls=[]\n",
    "    base_url=\"https://github.com\"\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url + tag[\"href\"])\n",
    "    return topic_urls\n",
    "def scrape_topics():\n",
    "    topics_url=(\"https://github.com/topics\")\n",
    "    response=requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page{}'.format(topic_url))\n",
    "    topics_dict={\n",
    "        'title':get_topic_titles(soup),\n",
    "        'description':get_topic_descs(soup),\n",
    "        'url':get_topic_urls(soup)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)\n",
    "   \n",
    "def scrape_topics_repos():\n",
    "    print(\"Sraping list of topics:\")\n",
    "    topics_df=scrape_topics()\n",
    "    for index,row in topics_df.iterrows():\n",
    "        print('scraping top repositories for\"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'],row['title'])\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Future work:\n",
    "summary \n",
    "- From github get all topics from topic page\n",
    "- then get all the top repos from a paerticular topic page\n",
    "- information of top repos includes repository name , username, repository link, stars "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "- from BeautifulSoup documentation\n",
    "- youtube\n",
    "- pandas documentation (for making datasets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future work\n",
    "\n",
    "- will get all the information for trending repositories and will explore more repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
